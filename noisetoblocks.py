# -*- coding: utf-8 -*-
"""NoiseToBlocks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PDt-2LEen7bah5jokUMXkWXgScoonJUL
"""

#@title Default title text
import numpy as np
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.layers.advanced_activations import LeakyReLU
from keras.layers import dot
import matplotlib.pyplot as plt
import pandas as pd

# Load data from public location
mydata = pd.read_csv('https://www.dropbox.com/s/105nkylqfa1uku7/small_wide.csv?dl=1')
mydata = mydata.drop(['npi'], axis=1)
mydata = mydata.as_matrix()
mydata = np.log(1+mydata)
mydata = mydata.transpose()

N = mydata.shape[0]
K_data = mydata.shape[1]

epoch_rounds = 100

# Generate data
# N = 1000
# K_data = 1000
# N_blocks = 100
# max_block_height = 100
# max_block_width = 50
# mydata = np.zeros((N,K_data))
# for i in range(N_blocks):                                                                                      
#     xlb =  np.random.randint(mydata.shape[0])                                                                                    
#     xub = np.minimum(xlb + 1 + np.random.randint(max_block_height), mydata.shape[0])                                             
#     ylb =  np.random.randint(mydata.shape[1])                                                                                     
#     yub = np.minimum(ylb + 1 + np.random.randint(max_block_width), mydata.shape[1])                                                                                                                                                                         
#     mydata[xlb:xub,ylb:yub] = np.random.poisson(np.random.randint(10), size=(xub-xlb, yub-ylb))
plt.imshow(mydata, aspect='auto')
plt.colorbar()
plt.show()


# In[2]:


# Let's model the block data from random noise.
K_noise = 128
hidden_size = 128

noise = np.random.randn(N, K_noise)

# In[3]:


# Define the model architecture
model = Sequential()
model.add(Dense(hidden_size, input_dim=K_noise, activation='tanh'))
model.add(Dense(hidden_size, activation='tanh'))
model.add(Dense(hidden_size, activation='linear'))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(K_data, activation='elu'))
print(model.summary())

# Define the optimization
model.compile(loss='mse', optimizer='Adam')

# Run the model
for i in range(epoch_rounds):
    model.fit(noise, mydata, epochs=1, batch_size=32,verbose=1)
    model.fit(noise, mydata, epochs=9, batch_size=32,verbose=0)

# Plot the result (prediction left, data right)
plt.subplot(121)
plt.imshow(model.predict(noise), aspect='auto')
plt.clim(0,20)
plt.subplot(122)
plt.imshow(mydata, aspect='auto')
plt.clim(0,20)
plt.show()

np.power((model.predict(noise) - mydata),2).sum()/(mydata.shape[0]*mydata.shape[1]-1)  # estimated MSE


# abigmatrix = np.random.randn(int(1e6), int(1e4))

# !cat /proc/cpuinfo
# !cat /proc/meminfo
# !df -h

# print(model.layers[-2].output)

# Some magic to get middle output of model from:  https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer
from keras import backend as K
from keras.models import Model
from keras import layers

def G(model, noise):
    inp = model.input                                           # input placeholder
    outputs = [layer.output for layer in model.layers]          # all layer outputs
    functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function

    # Testing
    layer_outs = functor([noise, 1.])
    return layer_outs[-2]

def GLastLayer(model2, x):
#     model2 = keras.models.clone_model(model)
#     config = layerm1.get_config()
#     layer = layers.deserialize({'class_name': layerm1.__class__.__name__,
#                         'config': config})
#     layer.set_weights(layerm1.get_weights())
#     inter1 = layerm2(K.variable(x))
    inter2 = model2.layers[-1](K.variable(x))
    return K.eval(inter2)

def GGetACloneForLastLayer(model):
    model2 = keras.models.clone_model(model)
    model2.layers[-1].set_weights(model.layers[-1].get_weights())
    return model2

# inside = GLastLayer(model, G(model,noise))
# print(K.eval(inside)[:5,:5], model.predict(noise)[:5,:5])  # passed

K_centers = 40
k_iterations = 10
noise_inside = G(model, noise)  # used to relocate centers
centers_noise = np.random.randn(K_centers, K_noise)
centers_inside = G(model, centers_noise)
centers = model.predict(centers_noise)  # K x o_width, K x h

# np.tile(np.expand_dims(noise_inside,0), (2,1,1)).shape:
modelClone = GGetACloneForLastLayer(model)

dists_to_centers = \
    (np.abs(np.expand_dims(mydata,0) - np.expand_dims(centers,1))).mean(2)
assignments = np.argmin(dists_to_centers,0)  # hard assignments

for k_index in range(k_iterations):
    if k_index % 10 == 0:
        print('Epoch' + str(k_index))
    newcenters_inside = []
    for assignment in np.unique(assignments):
        temp = noise_inside[np.where(assignment == assignments)[0],:]
        result = temp.mean(0)  # l2 averaging
#         result = np.power((temp*temp).sum(0),0.5)  # hyperbolic #TODO check
#         norm = np.power((temp*temp).sum(1),0.5).mean()
#         result * norm / np.power(result.dot(result),0.5)
        newcenters_inside.append(np.expand_dims(result, 0))
    print("Centers for %d/%d" % (len(newcenters_inside), K_centers))
    if k_index == k_iterations - 1:
        break
    if K_centers - len(newcenters_inside) > 0:
        # newcenters.append(anchors_predata[np.random.choice(anchors_predata.size()[0], size=K - len(newcenters)),:])  # insert based on data
        new_noise = np.random.randn(K_centers-len(newcenters_inside),K_noise)
        newcenters_inside.append(G(model, new_noise))  # insert randomly according to GAN
        # TODO insert according to cluster impurity
    centers_inside = np.concatenate(newcenters_inside, axis=0)
    centers = GLastLayer(modelClone, centers_inside)
    dists_to_centers = \
        np.abs(np.expand_dims(mydata,0) - np.expand_dims(centers,1)).mean(2)
    assignments = np.argmin(dists_to_centers,0)  # hard assignments

# len(np.unique(assignments))
# print(assignments.shape, mydata.shape)
np.bincount(assignments)

image_w_cluster = mydata[:,np.random.permutation(mydata.shape[1])]
image_ordered = image_w_cluster[np.argsort(assignments),:]

plt.subplot(121)
plt.imshow(image_ordered, aspect='auto')
plt.xlabel('Cluster membership')
plt.subplot(122)
plt.imshow(np.expand_dims(assignments[np.argsort(assignments)],1),aspect='auto')
plt.colorbar()
plt.show()

# assignments = np.random.choice(mydata.shape[0], size=mydata.shape[0], replace=True)
mydf = pd.DataFrame(mydata)
mydf['assignments'] = assignments
row_assignments = mydf.groupby(assignments).sum().drop('assignments', 1).as_matrix()

# print(mydata.shape, row_assignments.shape)
mydata2 = np.concatenate((image_ordered, row_assignments), 0).transpose()
mydata2.shape

mydf.groupby(assignments).sum()
# print(noise2.shape,mydata2.shape, row_assignments.shape)

# Define the noise vector
noise2 = np.random.randn(mydata2.shape[0], K_noise)

# Define the model architecture
model2 = Sequential()
model2.add(Dense(hidden_size, input_dim=K_noise, activation='tanh'))
model2.add(Dense(hidden_size, activation='tanh'))
model2.add(Dense(hidden_size, activation='linear'))
model2.add(LeakyReLU(alpha=0.1))
model2.add(Dense(mydata2.shape[1], activation='elu'))
print(model2.summary())

# Define the optimization
model2.compile(loss='mse', optimizer='Adam')


# Run the model
for i in range(epoch_rounds):
    model2.fit(noise2, mydata2, epochs=1, batch_size=32,verbose=1)
    model2.fit(noise2, mydata2, epochs=9, batch_size=32,verbose=0)

# Get (axis 1: now row) centers
K_centers = 100
k_iterations = 10
noise_inside = G(model2, noise2)  # used to relocate centers
centers2_noise = np.random.randn(K_centers, K_noise)
centers2_inside = G(model2, centers2_noise)
centers2 = model2.predict(centers2_noise)  # K x o_width, K x h


model2Clone = GGetACloneForLastLayer(model2)

dists_to_centers = \
    (np.abs(np.expand_dims(mydata2,0) - np.expand_dims(centers2,1))).mean(2)
assignments2 = np.argmin(dists_to_centers,0)  # hard assignments

for k_index in range(k_iterations):
    if k_index % 10 == 0:
        print('Epoch' + str(k_index))
    newcenters2_inside = []
    for assignment in np.unique(assignments2):
        temp = noise_inside[np.where(assignment == assignments2)[0],:]
        result = temp.mean(0)  # l2 averaging
#         result = np.power((temp*temp).sum(0),0.5)  # hyperbolic #TODO check
#         norm = np.power((temp*temp).sum(1),0.5).mean()
#         result * norm / np.power(result.dot(result),0.5)
        newcenters2_inside.append(np.expand_dims(result, 0))
    print("Centers for %d/%d" % (len(newcenters2_inside), K_centers))
    if k_index == k_iterations - 1:
        break
    if K_centers - len(newcenters2_inside) > 0:
        # newcenters.append(anchors_predata[np.random.choice(anchors_predata.size()[0], size=K - len(newcenters)),:])  # insert based on data
        new_noise = np.random.randn(K_centers-len(newcenters2_inside),K_noise)
        newcenters2_inside.append(G(model2, new_noise))  # insert randomly according to GAN
        # TODO insert according to cluster impurity
    centers2_inside = np.concatenate(newcenters2_inside, axis=0)
    centers2 = GLastLayer(model2Clone, centers2_inside)
    dists_to_centers = \
        np.abs(np.expand_dims(mydata2,0) - np.expand_dims(centers2,1)).mean(2)
    assignments2 = np.argmin(dists_to_centers,0)  # hard assignments

print(np.bincount(assignments2))

image2_ordered = mydata2[np.argsort(assignments2),:]

# Plot
plt.subplot(121)
plt.imshow(image2_ordered, aspect='auto')
plt.xlabel('Cluster membership')
plt.clim(0,8)
plt.subplot(122)
plt.imshow(np.expand_dims(assignments2[np.argsort(assignments2)],1),aspect='auto')
plt.colorbar()
plt.show()

image_ordered.mean(1)

# TODO add cluster memberships to plot
# TODO extract cluster members
# TODO compare to baseline, extract interpretation.

# Compare against truth (inspect the anomaly)

